
import boto3
import pandas as pd
from io import StringIO

# Configurar la sesión de AWS 
s3_client = boto3.client('s3', region_name='us-east-1')  

# Función para leer archivos CSV desde S3
def read_s3_csv(bucket_name, file_key):
    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)
    return pd.read_csv(obj['Body'])

# Nombre del bucket y claves de los archivos CSV
bucket_name = 'grupo-17-mlops-bucket'  # Nombre del bucket
ads_views_key = 'ads_views.csv'  # Ruta del archivo en el bucket
advertiser_ids_key = 'advertiser_ids.csv'  # Ruta del archivo en el bucket
product_views_key = 'product_views.csv'  # Ruta del archivo en el bucket

# Cargar los CSVs directamente desde S3
ads_views = read_s3_csv(bucket_name, ads_views_key)
advertiser_ids = read_s3_csv(bucket_name, advertiser_ids_key)
product_views = read_s3_csv(bucket_name, product_views_key)

# Mostrar las primeras filas de cada DataFrame para asegurarnos de que se cargaron correctamente
print("ads_views:")
print(ads_views.head())

print("\nadvertiser_ids:")
print(advertiser_ids.head())

print("\nproduct_views:")
print(product_views.head())

# Filtrar los logs para que solo contengan advertisers activos
ads_views_filtered = ads_views[ads_views['advertiser_id'].isin(advertiser_ids['advertiser_id'])]
product_views_filtered = product_views[product_views['advertiser_id'].isin(advertiser_ids['advertiser_id'])]

# Guardar los resultados filtrados en S3 en una carpeta 'Datos filtrados/'
output_ads_views_key = 'Datos filtrados/ads_views_filtered.csv'  # Ruta de salida en el bucket
output_product_views_key = 'Datos filtrados/product_views_filtered.csv'  # Ruta de salida en el bucket

# Convertir los DataFrames filtrados a CSV en un objeto StringIO y luego cargar a S3
csv_ads_views = StringIO()
ads_views_filtered.to_csv(csv_ads_views, index=False)
csv_ads_views.seek(0)  # Volver al principio del archivo en memoria

csv_product_views = StringIO()
product_views_filtered.to_csv(csv_product_views, index=False)
csv_product_views.seek(0)  # Volver al principio del archivo en memoria

# Subir los archivos filtrados de nuevo a S3
s3_client.put_object(Body=csv_ads_views.getvalue(), Bucket=bucket_name, Key=output_ads_views_key)
s3_client.put_object(Body=csv_product_views.getvalue(), Bucket=bucket_name, Key=output_product_views_key)

print("Archivos filtrados guardados en S3.")


#Subo el archivo Python a Github
git add process_s3_data.py
git commit -m "Agregar script para procesar datos desde S3"
git push
