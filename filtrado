from airflow import DAG
from airflow.operators.python import PythonOperator  # Nota: este es el módulo actualizado
from datetime import datetime
import sys

# Asegurarse de que Airflow puede encontrar tu script
sys.path.append('/home/ubuntu/Trabajo-Practico-MLOPS')  # Cambia esta ruta si es diferente

# Importar las funciones del script de filtrado
from filtrado import filter_ads_views, filter_product_views, save_to_s3

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 12, 1),
    'retries': 1,
}

with DAG(
    'filtrado_datos_dag',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
) as dag:

    def ejecutar_filtrado():
        from filtrado import ads_views, advertiser_ids, product_views  # Mueve esta importación dentro de la función
        ads_views_filtered = filter_ads_views(ads_views, advertiser_ids)
        product_views_filtered = filter_product_views(product_views, advertiser_ids)

        # Guardar resultados en S3
        bucket_name = 'grupo-17-mlops-bucket'
        save_to_s3(ads_views_filtered, bucket_name, 'Datos_filtrados/ads_views_filtered.csv')
        save_to_s3(product_views_filtered, bucket_name, 'Datos_filtrados/product_views_filtered.csv')
        print("Filtrado completado y datos guardados en S3.")

    tarea_filtrar_datos = PythonOperator(
        task_id='filtrar_datos_task',
        python_callable=ejecutar_filtrado,
    )
