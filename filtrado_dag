from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
import boto3
import pandas as pd
from io import StringIO

# Configurar la sesión de AWS
s3_client = boto3.client('s3', region_name='us-east-1')

# Función para leer archivos CSV desde S3
def read_s3_csv(bucket_name, file_key):
    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)
    return pd.read_csv(obj['Body'])

# Función para filtrar ads_views
def filter_ads_views(**kwargs):
    bucket_name = 'grupo-17-mlops-bucket'
    ads_views_key = 'ads_views.csv'
    advertiser_ids_key = 'advertiser_ids.csv'

    # Leer datos de S3
    ads_views = read_s3_csv(bucket_name, ads_views_key)
    advertiser_ids = read_s3_csv(bucket_name, advertiser_ids_key)

    # Filtrar datos
    df = ads_views[ads_views['advertiser_id'].isin(advertiser_ids['advertiser_id'])]
    df = df[df['date'] == datetime.today().strftime('%Y-%m-%d')]

    # Guardar datos filtrados en S3
    output_ads_views_key = 'Datos filtrados/ads_views_filtered.csv'
    csv_ads_views = StringIO()
    df.to_csv(csv_ads_views, index=False)
    csv_ads_views.seek(0)
    s3_client.put_object(Body=csv_ads_views.getvalue(), Bucket=bucket_name, Key=output_ads_views_key)

def filter_product_views(**kwargs):
    bucket_name = 'grupo-17-mlops-bucket'
    product_views_key = 'product_views.csv'
    advertiser_ids_key = 'advertiser_ids.csv'

    # Leer datos de S3
    product_views = read_s3_csv(bucket_name, product_views_key)
    advertiser_ids = read_s3_csv(bucket_name, advertiser_ids_key)

    # Filtrar datos
    advertisers = advertiser_ids['advertiser_id'].tolist()
    df = product_views[product_views['advertiser_id'].isin(advertisers)]
    df = df[df['date'] == datetime.today().strftime('%Y-%m-%d')]

    # Guardar datos filtrados en S3
    output_product_views_key = 'Datos filtrados/product_views_filtered.csv'
    csv_product_views = StringIO()
    df.to_csv(csv_product_views, index=False)
    csv_product_views.seek(0)
    s3_client.put_object(Body=csv_product_views.getvalue(), Bucket=bucket_name, Key=output_product_views_key)

# Definir el DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
}

with DAG(
    'filtrado_datos_dag',
    default_args=default_args,
    description='DAG para filtrar datos y guardarlos en S3',
    schedule_interval='@daily',
    start_date=datetime(2023, 12, 1),
    catchup=False,
) as dag:

    task_filter_ads_views = PythonOperator(
        task_id='filter_ads_views',
        python_callable=filter_ads_views,
    )

    task_filter_product_views = PythonOperator(
        task_id='filter_product_views',
        python_callable=filter_product_views,
    )

    task_filter_ads_views >> task_filter_product_views
